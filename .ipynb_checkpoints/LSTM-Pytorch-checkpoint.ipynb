{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data set preprocessing\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "Reviews_0 = []\n",
    "Reviews_1 = []\n",
    "\n",
    "with open( 'Orlando_complete.csv', 'r' ) as f:\n",
    "    Reader = csv.reader( f, delimiter=',', quoting=csv.QUOTE_MINIMAL )\n",
    "    for record in Reader:\n",
    "        \n",
    "        # Retrieve reviews with images\n",
    "        if len( record[ 10]) == 0:\n",
    "            \n",
    "            # review content\n",
    "            content = record[ 3 ].decode( 'utf-8' ) + ' ' + record[ 6 ].decode('utf-8')\n",
    "            content = content.replace( '<br>', '' )\n",
    "            content = content.replace( '</br>', '' )\n",
    "            content = content.lower()\n",
    "        \n",
    "            # review length\n",
    "            length = len( word_tokenize( content ) )\n",
    "        \n",
    "            # review helpfulness label\n",
    "            if float( record[ 7 ] ) >= 4.0:\n",
    "                label = 1\n",
    "            elif float( record[ 7 ] ) <= 2.0:\n",
    "                label = 0\n",
    "            \n",
    "            # image files\n",
    "            img_files = record[ 10 ]\n",
    "            \n",
    "            # rebuid the dataset for further processing\n",
    "            temp = [ content.encode('utf-8'), label, length, img_files ]\n",
    "            if label == 1:\n",
    "                Reviews_1.append( temp )\n",
    "            elif label == 0:\n",
    "                Reviews_0.append( temp )\n",
    "\n",
    "# Shuffle the reviews\n",
    "if len( Reviews_1 ) > len( Reviews_0 ):\n",
    "    temp = random.sample( Reviews_1, len( Reviews_0 ) )\n",
    "    Reviews = Reviews_0 + temp \n",
    "else:\n",
    "    temp = random.sample( Reviews_0, len( Reviews_1 ) )\n",
    "    Reviews = Reviews_1 + temp\n",
    "    \n",
    "data_size = len( Reviews )\n",
    "np.random.shuffle( Reviews )\n",
    "\n",
    "# Spilt into train, valid, test - 60%, 20%, 20%\n",
    "train_size = int( data_size * 0.6 )\n",
    "valid_size = int( data_size * 0.2 )\n",
    "\n",
    "train = Reviews[ :train_size ]\n",
    "valid = Reviews[ train_size:train_size + valid_size ]\n",
    "test = Reviews[ train_size + valid_size:]\n",
    "\n",
    "# Sort the reviews in descending order\n",
    "train.sort( key=lambda x:x[2], reverse=True )\n",
    "valid.sort( key=lambda x:x[2], reverse=True )\n",
    "test.sort( key=lambda x:x[2], reverse=True )\n",
    "\n",
    "# Save into different files\n",
    "split = [ 'train', 'valid', 'test' ]\n",
    "for data_src in split:\n",
    "    with open(  'data/trip/' + data_src + '.csv', 'w+' ) as f:\n",
    "        Writer = csv.writer( f, delimiter=',', quoting=csv.QUOTE_MINIMAL )\n",
    "        \n",
    "        if data_src == 'train':\n",
    "            Records = train\n",
    "        elif data_src == 'valid':\n",
    "            Records = valid\n",
    "        else:\n",
    "            Records = test\n",
    "            \n",
    "        for record in Records:\n",
    "            record = [ str( item ) for item in record ]\n",
    "            Writer.writerow( record )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained GloVe Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pretrained GloVe word vectors - 200d\n",
    "Word2vec_dic = {}\n",
    "with open( 'word2vecs/glove.6B/glove.6B.200d.txt', 'r' ) as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        vector = line[1:]\n",
    "        vector = [ float( item ) for item in vector ]\n",
    "        Word2vec_dic[ word ] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "##########################################################################\n",
    "#                         CommandLine Argument Setup                     #\n",
    "##########################################################################\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch RNN/LSTM Helpful Prediction Model')\n",
    "\n",
    "parser.add_argument( '-f', default='self', help='To make it runnable in jupyter' )\n",
    "\n",
    "parser.add_argument('--data', type=str, default='./data/No-photo-comparable/trip/',\n",
    "                    help='location of the data corpus')\n",
    "\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
    "\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "\n",
    "parser.add_argument('--nlayers', type=int, default=1,\n",
    "                    help='number of layers')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1e-2,\n",
    "                    help='initial learning rate')\n",
    "\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                   help='weight decay factor')\n",
    "\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                   help='sgd with momentum')\n",
    "\n",
    "parser.add_argument('--clip', type=float, default=0.5,\n",
    "                    help='gradient clipping')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='upper epoch limit')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "\n",
    "parser.add_argument('--bptt', type=int, default=20,\n",
    "                    help='sequence length')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "\n",
    "parser.add_argument('--cuda', default=True, action='store_true',\n",
    "                    help='use CUDA')\n",
    "\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='report interval')\n",
    "\n",
    "parser.add_argument('--save', type=str,  default='model.pt',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Class\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class Dictionary( object ):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append( word )\n",
    "            self.word2idx[ word ] = len(self.idx2word) - 1\n",
    "        return self.word2idx[ word ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.idx2word )\n",
    "\n",
    "\n",
    "class Corpus( object ):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train, self.train_len, self.train_label = self.tokenize( os.path.join( path, 'train.csv' ) )\n",
    "        self.valid, self.valid_len, self.valid_label = self.tokenize( os.path.join( path, 'valid.csv' ) )\n",
    "        self.test, self.test_len, self.test_label = self.tokenize( os.path.join( path, 'test.csv' ) )\n",
    "        self.test_on_image, self.test_on_image_len, self.test_on_image_label = self.tokenize( \n",
    "            os.path.join( path, 'test_on_image.csv' ) )\n",
    "\n",
    "    def tokenize( self, path ):\n",
    "        \"\"\"Tokenizes a csv file.\"\"\"\n",
    "        assert os.path.exists( path )\n",
    "        # Add words to the dictionary\n",
    "        max_len = 0\n",
    "        with open( path, 'r' ) as f:\n",
    "            Reader = csv.reader( f, delimiter=',', quoting=csv.QUOTE_MINIMAL )\n",
    "            count = 0\n",
    "            for record in Reader:\n",
    "                count += 1\n",
    "                words = word_tokenize( record[ 0 ].decode('utf-8') )\n",
    "                if len( words ) > max_len:\n",
    "                    max_len = len( words )\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open( path, 'r' ) as f:\n",
    "            Reader = csv.reader( f, delimiter=',', quoting=csv.QUOTE_MINIMAL )\n",
    "            data = torch.LongTensor( count, max_len ).fill_( 0 )\n",
    "            lengths = []\n",
    "            labels = torch.LongTensor( count ).fill_( 0 )\n",
    "\n",
    "            for idx, record in enumerate( Reader ):\n",
    "                labels[ idx ] = int( record[ 1 ] )\n",
    "                words = word_tokenize( record[ 0 ].decode('utf-8') )\n",
    "                lengths.append( len( words ) )\n",
    "                \n",
    "                for i, word in enumerate( words ):\n",
    "                    data[ idx, i ] = self.dictionary.word2idx[word]\n",
    "                    \n",
    "        return data, lengths, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the corpus and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = Corpus( args.data )\n",
    "\n",
    "# Batchify the whole dataset\n",
    "def select_data( data, bsz ):\n",
    "    try:\n",
    "        nbatch = data.size( 0 ) // bsz\n",
    "        data = data.narrow( 0, 0, nbatch * bsz )\n",
    "    \n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "    except:\n",
    "        nbatch = len( data ) // bsz\n",
    "        data = data[ : nbatch * bsz ]\n",
    "        \n",
    "    return data\n",
    "\n",
    "eval_batch_size = 20\n",
    "\n",
    "# Make Dataset batchifiable\n",
    "train_data = select_data( corpus.train, args.batch_size )\n",
    "train_len = select_data( corpus.train_len, args.batch_size )\n",
    "train_label = select_data( corpus.train_label, args.batch_size )\n",
    "\n",
    "val_data = select_data( corpus.valid, eval_batch_size )\n",
    "val_len = select_data( corpus.valid_len, eval_batch_size )\n",
    "val_label = select_data( corpus.valid_label, eval_batch_size )\n",
    "\n",
    "test_data = select_data( corpus.test, eval_batch_size )\n",
    "test_len = select_data( corpus.test_len, eval_batch_size )\n",
    "test_label = select_data( corpus.test_label, eval_batch_size )\n",
    "\n",
    "test_on_image_data = select_data( corpus.test_on_image, eval_batch_size )\n",
    "test_on_image_len = select_data( corpus.test_on_image_len, eval_batch_size )\n",
    "test_on_image_label = select_data( corpus.test_on_image_label, eval_batch_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Word Embedding Initialization Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntokens = len( corpus.dictionary )\n",
    "emb_matrix = torch.FloatTensor( ntokens, args.emsize )\n",
    "word_idx_list = []\n",
    "initrange = 0.1\n",
    "for idx in range( ntokens ):\n",
    "    try:\n",
    "        vec = Word2vec_dic[ corpus.dictionary.idx2word[ idx ] ]\n",
    "        emb_matrix[ idx ] = torch.FloatTensor( vec )\n",
    "    except:\n",
    "        word_idx_list.append( idx )\n",
    "        vec = torch.FloatTensor( 1, args.emsize )\n",
    "        vec.uniform_( -initrange, initrange )\n",
    "        emb_matrix[ idx ] = vec\n",
    "\n",
    "# Get Index of Word Embedding that need to be updated during training\n",
    "if args.cuda:\n",
    "    word_idx_list = torch.cuda.LongTensor( word_idx_list )\n",
    "else:\n",
    "    word_idx_list = torch.LongTensor( word_idx_list )\n",
    "    \n",
    "torch.save( emb_matrix, 'data/No-photo-comparable/trip/emb_matrix-with-image.pt' )\n",
    "torch.save( word_idx_list, 'data/No-photo-comparable/trip/word_idx_list-with-image.pt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__( self, rnn_type, ntoken, ninp, nhid, nlayers, nclass, emb_matrix ):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.encoder = nn.Embedding( ntoken, ninp )\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)( ninp, nhid, nlayers, bias=False )\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN( ninp, nhid, nlayers, nonlinearity=nonlinearity, bias=False )\n",
    "            \n",
    "        self.decoder = nn.Linear( nhid, nclass )\n",
    "\n",
    "        self.init_wordembedding( emb_matrix )\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.fill_( 0 )\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_wordembedding( self, embedding_matrix ):\n",
    "        self.encoder.weight.data = embedding_matrix\n",
    "\n",
    "    def forward(self, input, hidden, len_li ):\n",
    "        emb = self.encoder( input )\n",
    "        \n",
    "        rnn_input = torch.nn.utils.rnn.pack_padded_sequence( emb, list( len_li.data ) )\n",
    "        output, hidden = self.rnn( rnn_input , hidden )\n",
    "        \n",
    "        depacked_output, _ = torch.nn.utils.rnn.pad_packed_sequence( output )\n",
    "            \n",
    "        # Select the last token's hidden state embedding\n",
    "        idx = ( len_li - 1 ).view( -1 , 1 ).expand( depacked_output.size( 1 ), depacked_output.size( 2 ) ).unsqueeze( 0 )\n",
    "        last_output = depacked_output.gather( 0, idx ).squeeze()\n",
    "        \n",
    "        decoded = self.decoder( last_output )\n",
    "        \n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden( self, bsz ):\n",
    "        weight = next( self.parameters() ).data\n",
    "        # if it's LSTM, we will have long term and short term hidden states\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return ( Variable( weight.new( self.nlayers, bsz, self.nhid ).zero_() ),\n",
    "                     Variable( weight.new( self.nlayers, bsz, self.nhid ).zero_() ) )\n",
    "        else:\n",
    "            return Variable( weight.new( self.nlayers, bsz, self.nhid).zero_() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ntokens = len( corpus.dictionary )\n",
    "nclass = 2\n",
    "\n",
    "model = RNNModel( args.model, ntokens, args.emsize, args.nhid, args.nlayers, nclass, emb_matrix )\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unpack the hidden state\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type( h ) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple( repackage_hidden(v) for v in h )\n",
    "\n",
    "# Retrieve a batch from the source\n",
    "def get_batch( source, labels, len_list, i, size, evaluation=False ):\n",
    "    \n",
    "    batch_size = size\n",
    "    data = source[ i : i + batch_size ].t()\n",
    "    data = Variable( data , volatile = evaluation )\n",
    "    target = Variable( labels[ i : i + batch_size ].view( -1 ) )\n",
    "    len_li = torch.LongTensor( len_list[ i : i + batch_size ] )\n",
    "    if args.cuda:\n",
    "        len_li = Variable( len_li.cuda() )\n",
    "    else:\n",
    "        len_li = Variable( len_li )\n",
    "    \n",
    "    return data, target, len_li\n",
    "\n",
    "def update_embedding( word_indice, lr ):\n",
    "    model.encoder.weight.data[ word_indice ] -= lr * model.encoder.weight.grad.data[ word_indice ]\n",
    "\n",
    "# Define the training function\n",
    "def train( lr, word_update=False ):\n",
    "    # word_update: whether glove vectors are updated\n",
    "\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    all_losses = []\n",
    "\n",
    "    hidden = model.init_hidden( args.batch_size )\n",
    "    \n",
    "    # Per-parameter training\n",
    "    if not word_update:\n",
    "        parameter_list = [ { 'params': model.encoder.parameters(), 'lr':0 }, \n",
    "                          { 'params': model.rnn.parameters() }, { 'params': model.decoder.parameters()  } ]\n",
    "    else:\n",
    "        parameter_list = [ { 'params': model.parameters() } ]\n",
    "    \n",
    "    optimizer = torch.optim.Adam( parameter_list, lr, weight_decay = args.weight_decay )\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate( range( 0, train_data.size(0) - 1, args.batch_size ) ):\n",
    "        \n",
    "        # Retrieve one batch for training\n",
    "        data, targets, len_li = get_batch( train_data, train_label, train_len, start_idx, args.batch_size )\n",
    "        hidden = repackage_hidden( hidden )\n",
    "        \n",
    "        output, hidden = model( data, hidden, len_li )\n",
    "        loss = criterion( output.view( -1, nclass ), targets )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm( model.parameters(), args.clip )\n",
    "        \n",
    "        optimizer.step()\n",
    "        # Only update words not in Glove\n",
    "        if not word_update:\n",
    "            update_embedding( word_idx_list, lr )\n",
    "        \n",
    "        total_loss += loss.data\n",
    "        all_losses.append( loss.data )\n",
    "\n",
    "\n",
    "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
    "            cur_loss = total_loss[0] / args.log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} |'.format(\n",
    "                epoch, batch_idx, len( train_data ) // args.batch_size, lr,\n",
    "                elapsed * 1000 / args.log_interval, cur_loss ) )\n",
    "                  \n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        break\n",
    "\n",
    "    return np.mean( all_losses )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute precision, recall, f1 and accuracy\n",
    "def compute_measure( pred, target ):\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    tn, fp, fn, tp = 0, 0, 0, 0\n",
    "    for i in range( pred.size(0) ):\n",
    "        if pred.data[ i ] == 1 and target.data[ i ] == 1:\n",
    "            tp += 1\n",
    "        elif pred.data[ i ] == 1 and target.data[ i ] == 0:\n",
    "            fp += 1\n",
    "        elif pred.data[ i ] == 0 and target.data[ i ] == 1:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    print pred, target       \n",
    "    print tp, tn, fp, fn\n",
    "    pre = tp / float( fp + tp )\n",
    "    rec = tp / float( fn + tp )\n",
    "    f1 = 2 * pre * rec / ( pre + rec )\n",
    "    acc = ( tn + tp ) / float( tn + fp + fn + tp )\n",
    "            \n",
    "    return pre, rec, f1, acc\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate( data_source, labels, data_len ):\n",
    "    total_loss = 0\n",
    "    \n",
    "    acc = []\n",
    "    pre = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    \n",
    "    ntokens = len( corpus.dictionary )\n",
    "    hidden = model.init_hidden( eval_batch_size )\n",
    "    \n",
    "    for i in range( 0, data_source.size(0) - 1, eval_batch_size ):\n",
    "        \n",
    "        data, targets, len_li = get_batch( data_source, labels, data_len, i, eval_batch_size, evaluation=True )\n",
    "        output, hidden = model( data, hidden, len_li )\n",
    "        output_flat = output.view( -1, nclass )\n",
    "        \n",
    "        total_loss += data.size( 1 ) * criterion( output_flat, targets ).data\n",
    "        hidden = repackage_hidden( hidden )\n",
    "        \n",
    "        _, pred = output_flat.topk( 1 , 1, True, True )\n",
    "        pred = pred.t()\n",
    "        target = target.view( 1, -1 )\n",
    "        \n",
    "        p, r, f, a = compute_measure( pred, target )\n",
    "        acc.append( a )\n",
    "        pre.append( p )\n",
    "        rec.append( r )\n",
    "        f1.append( f )\n",
    "    \n",
    "    # Compute Precision, Recall, F1, and Accuracy\n",
    "    print 'Measure on this dataset'\n",
    "    print 'Precision:', np.mean( pre )\n",
    "    print 'Recall:', np.mean( rec )\n",
    "    print 'F1:', np.mean( f1 )\n",
    "    print 'Acc:', np.mean( acc )\n",
    "\n",
    "    return total_loss[0] / len( data_source )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Start Epoch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy on validation dataset: Variable containing:\n",
      " 0.5015\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.45s | valid loss  2.50 | \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lr = args.lr\n",
    "prev_val_loss = None\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range( 1, args.epochs + 1 ):\n",
    "    epoch_start_time = time.time()\n",
    "    all_losses.append( train( lr, epoch )[0] )\n",
    "    val_loss = evaluate( val_data, val_label, val_len )\n",
    "    print( '-'*80 )\n",
    "    print( '| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          .format( epoch, (time.time() - epoch_start_time), val_loss ) )\n",
    "    print( '-'*80 )\n",
    "          \n",
    "    # Anneal the learning rate.\n",
    "    if prev_val_loss and val_loss > prev_val_loss:\n",
    "        lr /= 4\n",
    "    prev_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhkAAAFkCAYAAACNTikJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f809b6f6d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( all_losses )\n",
    "\n",
    "plt.savefig( 'Training_curve.eps' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run on test data and save the model.\n",
    "test_loss = evaluate( test_data, test_label, test_len )\n",
    "\n",
    "print('=' * 80)\n",
    "print( '| End of training | test loss {:5.2f} |'.format( test_loss ) )\n",
    "print('=' * 80)\n",
    "if args.save != '':\n",
    "    with open( args.save, 'wb' ) as f:\n",
    "        torch.save( model, f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained GloVe word vectors - 200d\n",
    "Word2vec_dic = {}\n",
    "with open( 'word2vecs/glove.6B/glove.6B.200d.txt', 'r' ) as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "        vector = line[1:]\n",
    "        vector = [ float( item ) for item in vector ]\n",
    "        Word2vec_dic[ word ] = vector\n",
    "\n",
    "# # Command Line Parameters Setup\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "##########################################################################\n",
    "#                         CommandLine Argument Setup                     #\n",
    "##########################################################################\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch RNN/LSTM Helpful Prediction Model')\n",
    "\n",
    "parser.add_argument( '-f', default='self', help='To make it runnable in jupyter' )\n",
    "\n",
    "parser.add_argument('--data', type=str, default='./data/review-photo/',\n",
    "                    help='location of the data corpus')\n",
    "\n",
    "parser.add_argument('--model', type=str, default='LSTM',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)')\n",
    "\n",
    "parser.add_argument('--emsize', type=int, default=200,\n",
    "                    help='size of word embeddings')\n",
    "\n",
    "parser.add_argument('--nhid', type=int, default=200,\n",
    "                    help='number of hidden units per layer')\n",
    "\n",
    "parser.add_argument('--nlayers', type=int, default=1,\n",
    "                    help='number of layers')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1e-2,\n",
    "                    help='initial learning rate')\n",
    "\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                   help='weight decay factor')\n",
    "\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                   help='sgd with momentum')\n",
    "\n",
    "parser.add_argument('--clip', type=float, default=0.5,\n",
    "                    help='gradient clipping')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='upper epoch limit')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='batch size')\n",
    "\n",
    "parser.add_argument('--bptt', type=int, default=20,\n",
    "                    help='sequence length')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "\n",
    "parser.add_argument('--cuda', default=True, action='store_true',\n",
    "                    help='use CUDA')\n",
    "\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='report interval')\n",
    "\n",
    "parser.add_argument('--save', type=str,  default='model-photo-text-yelp.pt',\n",
    "                    help='path to save the final model')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# # Load Data\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "# Data Class\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class Dictionary( object ):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append( word )\n",
    "            self.word2idx[ word ] = len(self.idx2word) - 1\n",
    "        return self.word2idx[ word ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len( self.idx2word )\n",
    "\n",
    "\n",
    "class Corpus( object ):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train, self.train_len, self.train_label = self.tokenize( os.path.join( path, 'train.csv' ) )\n",
    "        self.valid, self.valid_len, self.valid_label = self.tokenize( os.path.join( path, 'valid.csv' ) )\n",
    "        self.test, self.test_len, self.test_label = self.tokenize( os.path.join( path, 'test.csv' ) )\n",
    "\n",
    "    def tokenize( self, path ):\n",
    "        \"\"\"Tokenizes a csv file.\"\"\"\n",
    "        assert os.path.exists( path )\n",
    "        # Add words to the dictionary\n",
    "        max_len = 0\n",
    "        with open( path, 'r' ) as f:\n",
    "            Reader = csv.reader( f, delimiter=',', quoting=csv.QUOTE_MINIMAL )\n",
    "            count = 0\n",
    "            for record in Reader:\n",
    "                count += 1\n",
    "                words = word_tokenize( record[ 0 ].decode('utf-8') )\n",
    "                if len( words ) > max_len:\n",
    "                    max_len = len( words )\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open( path, 'r' ) as f:\n",
    "            Reader = csv.reader( f, delimiter=',', quoting=csv.QUOTE_MINIMAL )\n",
    "            data = torch.LongTensor( count, max_len ).fill_( 0 )\n",
    "            lengths = []\n",
    "            labels = torch.LongTensor( count ).fill_( 0 )\n",
    "\n",
    "            for idx, record in enumerate( Reader ):\n",
    "                labels[ idx ] = int( record[ 1 ] )\n",
    "                words = word_tokenize( record[ 0 ].decode('utf-8') )\n",
    "                lengths.append( len( words ) )\n",
    "                \n",
    "                for i, word in enumerate( words ):\n",
    "                    data[ idx, i ] = self.dictionary.word2idx[word]\n",
    "                    \n",
    "        return data, lengths, labels\n",
    "\n",
    "\n",
    "# # Define the corpus and load data\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "corpus = Corpus( args.data )\n",
    "print 'Training data loaded.'\n",
    "\n",
    "# Batchify the whole dataset\n",
    "def select_data( data, bsz ):\n",
    "    try:\n",
    "        nbatch = data.size( 0 ) // bsz\n",
    "        data = data.narrow( 0, 0, nbatch * bsz )\n",
    "    \n",
    "        if args.cuda:\n",
    "            data = data.cuda()\n",
    "    except:\n",
    "        nbatch = len( data ) // bsz\n",
    "        data = data[ : nbatch * bsz ]\n",
    "        \n",
    "    return data\n",
    "\n",
    "eval_batch_size = 20\n",
    "\n",
    "# Make Dataset batchifiable\n",
    "train_data = select_data( corpus.train, args.batch_size )\n",
    "train_len = select_data( corpus.train_len, args.batch_size )\n",
    "train_label = select_data( corpus.train_label, args.batch_size )\n",
    "\n",
    "val_data = select_data( corpus.valid, eval_batch_size )\n",
    "val_len = select_data( corpus.valid_len, eval_batch_size )\n",
    "val_label = select_data( corpus.valid_label, eval_batch_size )\n",
    "\n",
    "test_data = select_data( corpus.test, eval_batch_size )\n",
    "test_len = select_data( corpus.test_len, eval_batch_size )\n",
    "test_label = select_data( corpus.test_label, eval_batch_size )\n",
    "\n",
    "\n",
    "# # Setup Word Embedding Initialization Matrix\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "ntokens = len( corpus.dictionary )\n",
    "emb_matrix = torch.FloatTensor( ntokens, args.emsize )\n",
    "word_idx_list = []\n",
    "initrange = 0.1\n",
    "for idx in range( ntokens ):\n",
    "    try:\n",
    "        vec = Word2vec_dic[ corpus.dictionary.idx2word[ idx ] ]\n",
    "        emb_matrix[ idx ] = torch.FloatTensor( vec )\n",
    "    except:\n",
    "        word_idx_list.append( idx )\n",
    "        vec = torch.FloatTensor( 1, args.emsize )\n",
    "        vec.uniform_( -initrange, initrange )\n",
    "        emb_matrix[ idx ] = vec\n",
    "\n",
    "# Get Index of Word Embedding that need to be updated during training\n",
    "if args.cuda:\n",
    "    word_idx_list = torch.LongTensor( word_idx_list ).cuda()\n",
    "else:\n",
    "    word_idx_list = torch.LongTensor( word_idx_list )\n",
    "    \n",
    "torch.save( emb_matrix, 'data/review-photo/emb_matrix-text.pt' )\n",
    "torch.save( word_idx_list, 'data/review-photo/word_idx_list-text.pt' )\n",
    "\n",
    "# emb_matrix = torch.load( 'data/review-photo/emb_matrix-text.pt' )\n",
    "# word_idx_list = torch.load( 'data/review-photo/word_idx_list-text.pt' )\n",
    "\n",
    "# print 'Pretrained word2vec matrix loaded.'\n",
    "\n",
    "# # # Define the RNN Model\n",
    "\n",
    "# # In[6]:\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# class RNNModel(nn.Module):\n",
    "#     \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "#     def __init__( self, rnn_type, ntoken, ninp, nhid, nlayers, nclass, emb_matrix ):\n",
    "#         super(RNNModel, self).__init__()\n",
    "#         self.encoder = nn.Embedding( ntoken, ninp )\n",
    "#         if rnn_type in ['LSTM', 'GRU']:\n",
    "#             self.rnn = getattr(nn, rnn_type)( ninp, nhid, nlayers, bias=False )\n",
    "#         else:\n",
    "#             try:\n",
    "#                 nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "#             except KeyError:\n",
    "#                 raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "#                                  options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "#             self.rnn = nn.RNN( ninp, nhid, nlayers, nonlinearity=nonlinearity, bias=False )\n",
    "            \n",
    "#         self.decoder = nn.Linear( nhid, nclass )\n",
    "\n",
    "#         self.init_wordembedding( emb_matrix )\n",
    "#         self.init_weights()\n",
    "\n",
    "#         self.rnn_type = rnn_type\n",
    "#         self.nhid = nhid\n",
    "#         self.nlayers = nlayers\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.1\n",
    "#         self.decoder.bias.data.fill_( 0 )\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "#     def init_wordembedding( self, embedding_matrix ):\n",
    "#         self.encoder.weight.data = embedding_matrix\n",
    "\n",
    "#     def forward(self, input, hidden, len_li ):\n",
    "#         emb = self.encoder( input )\n",
    "        \n",
    "#         rnn_input = torch.nn.utils.rnn.pack_padded_sequence( emb, list( len_li.data ) )\n",
    "#         output, hidden = self.rnn( rnn_input , hidden )\n",
    "        \n",
    "#         depacked_output, _ = torch.nn.utils.rnn.pad_packed_sequence( output )\n",
    "            \n",
    "#         # Select the last token's hidden state embedding\n",
    "#         idx = ( len_li - 1 ).view( -1 , 1 ).expand( depacked_output.size( 1 ), depacked_output.size( 2 ) ).unsqueeze( 0 )\n",
    "#         last_output = depacked_output.gather( 0, idx ).squeeze()\n",
    "        \n",
    "#         decoded = self.decoder( last_output )\n",
    "        \n",
    "#         return decoded, hidden\n",
    "\n",
    "#     def init_hidden( self, bsz ):\n",
    "#         weight = next( self.parameters() ).data\n",
    "#         # if it's LSTM, we will have long term and short term hidden states\n",
    "#         if self.rnn_type == 'LSTM':\n",
    "#             return ( Variable( weight.new( self.nlayers, bsz, self.nhid ).zero_() ),\n",
    "#                      Variable( weight.new( self.nlayers, bsz, self.nhid ).zero_() ) )\n",
    "#         else:\n",
    "#             return Variable( weight.new( self.nlayers, bsz, self.nhid).zero_() )\n",
    "\n",
    "\n",
    "# # # Build the Model\n",
    "\n",
    "# # In[7]:\n",
    "\n",
    "# ntokens = len( corpus.dictionary )\n",
    "# nclass = 2\n",
    "\n",
    "# model = RNNModel( args.model, ntokens, args.emsize, args.nhid, args.nlayers, nclass, emb_matrix )\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# if args.cuda:\n",
    "#     model.cuda()\n",
    "#     criterion = criterion.cuda()\n",
    "\n",
    "\n",
    "# # # Training Code\n",
    "\n",
    "# # In[8]:\n",
    "\n",
    "# # Unpack the hidden state\n",
    "# def repackage_hidden(h):\n",
    "#     \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "#     if type( h ) == Variable:\n",
    "#         return Variable(h.data)\n",
    "#     else:\n",
    "#         return tuple( repackage_hidden(v) for v in h )\n",
    "\n",
    "# # Retrieve a batch from the source\n",
    "# def get_batch( source, labels, len_list, i, size, evaluation=False ):\n",
    "    \n",
    "#     batch_size = size\n",
    "#     data = source[ i : i + batch_size ].t()\n",
    "#     data = Variable( data , volatile = evaluation )\n",
    "#     target = Variable( labels[ i : i + batch_size ].view( -1 ) )\n",
    "#     len_li = torch.LongTensor( len_list[ i : i + batch_size ] )\n",
    "#     if args.cuda:\n",
    "#         len_li = Variable( len_li.cuda() )\n",
    "#     else:\n",
    "#         len_li = Variable( len_li )\n",
    "    \n",
    "#     return data, target, len_li\n",
    "\n",
    "# def update_embedding( word_indice, lr ):\n",
    "#     model.encoder.weight.data[ word_indice ] -= lr * model.encoder.weight.grad.data[ word_indice ]\n",
    "\n",
    "# # Define the training function\n",
    "# def train( lr, word_update=False ):\n",
    "#     # word_update: whether glove vectors are updated\n",
    "\n",
    "#     total_loss = 0\n",
    "#     start_time = time.time()\n",
    "#     all_losses = []\n",
    "\n",
    "#     hidden = model.init_hidden( args.batch_size )\n",
    "    \n",
    "#     # Per-parameter training\n",
    "#     if not word_update:\n",
    "#         parameter_list = [ { 'params': model.encoder.parameters(), 'lr':0 }, \n",
    "#                           { 'params': model.rnn.parameters() }, { 'params': model.decoder.parameters()  } ]\n",
    "#     else:\n",
    "#         parameter_list = [ { 'params': model.parameters() } ]\n",
    "    \n",
    "#     optimizer = torch.optim.Adam( parameter_list, lr, weight_decay = args.weight_decay )\n",
    "    \n",
    "#     for batch_idx, start_idx in enumerate( range( 0, train_data.size(0) - 1, args.batch_size ) ):\n",
    "        \n",
    "#         # Retrieve one batch for training\n",
    "#         data, targets, len_li = get_batch( train_data, train_label, train_len, start_idx, args.batch_size )\n",
    "#         hidden = repackage_hidden( hidden )\n",
    "        \n",
    "#         output, hidden = model( data, hidden, len_li )\n",
    "#         loss = criterion( output.view( -1, nclass ), targets )\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Gradient clipping for gradient explosion\n",
    "#         torch.nn.utils.clip_grad_norm( model.parameters(), args.clip )\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         # Only update words not in Glove\n",
    "#         if not word_update:\n",
    "#             update_embedding( word_idx_list, lr )\n",
    "        \n",
    "#         total_loss += loss.data\n",
    "#         all_losses.append( loss.data )\n",
    "\n",
    "\n",
    "#         if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
    "#             cur_loss = total_loss[0] / args.log_interval\n",
    "#             elapsed = time.time() - start_time\n",
    "#             print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "#                     'loss {:5.2f} |'.format(\n",
    "#                 epoch, batch_idx, len( train_data ) // args.batch_size, lr,\n",
    "#                 elapsed * 1000 / args.log_interval, cur_loss ) )\n",
    "                  \n",
    "#             total_loss = 0\n",
    "#             start_time = time.time()\n",
    "\n",
    "#     return np.mean( all_losses )\n",
    "\n",
    "\n",
    "# # # Evaluation Code\n",
    "\n",
    "# # Function to compute precision, recall, f1 and accuracy\n",
    "# def compute_measure( pred, target ):\n",
    "#     pred = pred.view(-1)\n",
    "#     target = target.view(-1)\n",
    "    \n",
    "#     tn, fp, fn, tp = 0, 0, 0, 0\n",
    "#     for i in range( pred.size(0) ):\n",
    "#         if pred.data[ i ] == 1 and target.data[ i ] == 1:\n",
    "#             tp += 1\n",
    "#         elif pred.data[ i ] == 1 and target.data[ i ] == 0:\n",
    "#             fp += 1\n",
    "#         elif pred.data[ i ] == 0 and target.data[ i ] == 1:\n",
    "#             fn += 1\n",
    "#         else:\n",
    "#             tn += 1\n",
    "\n",
    "#     pre = tp / float( fp + tp + 1e-8 )\n",
    "#     rec = tp / float( fn + tp + 1e-8 )\n",
    "#     f1 = 2 * pre * rec / ( pre + rec + 1e-8 )\n",
    "#     acc = ( tn + tp ) / float( tn + fp + fn + tp + 1e-8 )\n",
    "            \n",
    "#     return pre, rec, f1, acc\n",
    "\n",
    "# # Define evaluation function\n",
    "# def evaluate( data_source, labels, data_len ):\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     acc = []\n",
    "#     pre = []\n",
    "#     rec = []\n",
    "#     f1 = []\n",
    "    \n",
    "#     ntokens = len( corpus.dictionary )\n",
    "#     hidden = model.init_hidden( eval_batch_size )\n",
    "    \n",
    "#     for i in range( 0, data_source.size(0) - 1, eval_batch_size ):\n",
    "        \n",
    "#         data, targets, len_li = get_batch( data_source, labels, data_len, i, eval_batch_size, evaluation=True )\n",
    "#         output, hidden = model( data, hidden, len_li )\n",
    "#         output_flat = output.view( -1, nclass )\n",
    "        \n",
    "#         total_loss += data.size( 1 ) * criterion( output_flat, targets ).data\n",
    "#         hidden = repackage_hidden( hidden )\n",
    "        \n",
    "#         _, pred = output_flat.topk( 1 , 1, True, True )\n",
    "#         pred = pred.t()\n",
    "#         target = targets.view( 1, -1 )\n",
    "        \n",
    "#         p, r, f, a = compute_measure( pred, target )\n",
    "#         acc.append( a )\n",
    "#         pre.append( p )\n",
    "#         rec.append( r )\n",
    "#         f1.append( f )\n",
    "    \n",
    "#     # Compute Precision, Recall, F1, and Accuracy\n",
    "#     print 'Measure on this dataset'\n",
    "#     print 'Precision:', np.mean( pre )\n",
    "#     print 'Recall:', np.mean( rec )\n",
    "#     print 'F1:', np.mean( f1 )\n",
    "#     print 'Acc:', np.mean( acc )\n",
    "\n",
    "#     return total_loss[0] / len( data_source )\n",
    "\n",
    "\n",
    "# # # Start Epoch Training\n",
    "\n",
    "# # In[10]:\n",
    "\n",
    "# print 'Start training...'\n",
    "# lr = args.lr\n",
    "# prev_val_loss = None\n",
    "# all_losses = []\n",
    "\n",
    "# for epoch in range( 1, args.epochs + 1 ):\n",
    "#     epoch_start_time = time.time()\n",
    "#     all_losses.append( train( lr, epoch )[0] )\n",
    "#     val_loss = evaluate( val_data, val_label, val_len )\n",
    "#     print( '-'*80 )\n",
    "#     print( '| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "#           .format( epoch, (time.time() - epoch_start_time), val_loss ) )\n",
    "#     print( '-'*80 )\n",
    "          \n",
    "#     # Anneal the learning rate.\n",
    "#     if prev_val_loss and val_loss > prev_val_loss:\n",
    "#         lr /= 4\n",
    "#     prev_val_loss = val_loss\n",
    "\n",
    "\n",
    "# # # Plot the Learning Curves\n",
    "\n",
    "# # In[13]:\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot( all_losses )\n",
    "# plt.savefig( 'Training_curve_no-photo.eps' )\n",
    "\n",
    "\n",
    "# # # Test dataset Evaluation\n",
    "\n",
    "# # In[ ]:\n",
    "\n",
    "# # Run on test data and save the model.\n",
    "# test_loss = evaluate( test_data, test_label, test_len )\n",
    "\n",
    "# print( '=' * 80 )\n",
    "# print( '| End of training | test loss {:5.2f} |'.format( test_loss ) )\n",
    "# print( '=' * 80 )\n",
    "# if args.save != '':\n",
    "#     with open( args.save, 'wb' ) as f:\n",
    "#         torch.save( model, f )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
