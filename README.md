Self-Attentive Model

Implementation of A Structured Self-attentive Sentence Embedding [[Paper](https://arxiv.org/abs/1703.03130)] [[Review](https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/self_attention_embedding.md)]

To run the model, [GloVe](https://nlp.stanford.edu/projects/glove/) word vectors need to be downloaded and placed in 'word2vecs'. By default, we use 200d vectors trained 
on Wikipedia corpus.

``` shell
mkdir word2vecs
```

